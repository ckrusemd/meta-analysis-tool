{
 "cells": [
  {
   "source": [
    "# Grab Pubmed Abstracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrez_search_pubmed(query,records_per_query=10,email=\"XXX@YYY.com\",retMax=100):\n",
    "    from Bio import Entrez\n",
    "    Entrez.email = email\n",
    "    # Search\n",
    "    handle = Entrez.esearch(db=\"pubmed\",term=query, idtype=\"acc\", retMax=retMax)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_abstract(abstract_xml):\n",
    "    abstract = ''\n",
    "    for abstractText in abstract_xml.find_all('abstracttext'):\n",
    "        if abstractText.get('label') != None:\n",
    "            abstract = abstract + \" \" + abstractText.get('label') + \": \"\n",
    "        abstract = abstract + abstractText.text\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrez_fetch_abstracts(uid,email):\n",
    "    from Bio import Entrez\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    Entrez.email = email\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=uid, rettype='Medline', retmode='xml')\n",
    "    result = handle.readlines()\n",
    "    result = b\"\".join(result)\n",
    "    bs_content = bs(result, \"lxml\")\n",
    "    abstracts = bs_content.find_all('abstract')\n",
    "    handle.close()\n",
    "    # Abstract\n",
    "    results = [ flatten_abstract(abstract) for abstract in abstracts]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrez_construct_abstract_dict(uids,email):\n",
    "        results = entrez_fetch_abstracts( uids, email )\n",
    "        suitable = ['Yes', 'No']\n",
    "        suitable = np.random.choice(suitable, len(results), p=[0.7, 0.3])\n",
    "        return [{'text':b, 'sentiment':a} for a,b in zip(suitable, results)]"
   ]
  },
  {
   "source": [
    "## Query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_body={ \"query\": \"muscle hypertrophy\", 'email' : \"XXX@YYY.com\" }\n",
    "## Query search\n",
    "results_query = entrez_search_pubmed(query = query_body['query'], email = query_body['email'], retMax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "uids = results_query['IdList']\n",
    "len(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abstract\n",
    "results_abstracts = entrez_construct_abstract_dict(uids,\"XXX@YYY.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(results_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': ' BACKGROUND: Cross-education of strength refers to the strength gain that is transferred to the contralateral limb after a unilateral training program. HYPOTHESIS: Unilateral eccentric training using different muscle contraction times would improve the structural and functional properties of the untrained contralateral limb. STUDY DESIGN: Randomized controlled trial. LEVEL OF EVIDENCE: Level 2. METHODS: Thirty-six participants were randomized into a control group, experimental group 1 (EG6s; eccentric contraction runtime = 6 seconds) and experimental group 2 (EG3s; eccentric contraction runtime = 3 seconds). The thickness and elastographic index of the patellar tendon (PT), lean mass and fat percentage of the thigh, contractile properties of the vastus lateralis (VL), as well as isometric, concentric, and eccentric knee extensor peak torques, and eccentric single-leg decline squat (SLDSe) 1 repetition maximum (1-RM) were measured after 6 weeks of SLDSe training (3 times per week, 80% of 1-RM) and after 6 weeks of detraining in the untrained contralateral limb. RESULTS: After training, there was an increase in lean thigh mass of the untrained limb in both groups: EG6s (0.17 ± 0.29 kg;P = 0.03; effect size [ES] = 0.15) and EG3s (0.15 ± 0.23 kg; P = 0.04; ES = 0.19). Likewise, both EG6s (62.30 ± 19.09 kg; P < 0.001; ES = 4.23) and EG3s (68.09 ± 27.49 kg; P < 0.001; ES = 3.40) increased their 1-RM, isometric (EG6s: 48.64 ± 44.82 N·m, P < 0.001, ES = 0.63; EG3s: 34.81 ± 47.30 N·m, P = 0.004, ES = 0.38), concentric at 60 deg/s and 180 deg/s and eccentric at 60 deg/s and 180 deg/s knee extensor peak torques (P < 0.05) in the untrained limb. However, no differences were found in the contractile properties of the VL or in the thickness of the PT after eccentric training in either of the 2 experimental groups. CONCLUSION: Regardless of the runtime of the contraction, 6 weeks of SLSDe was effective for inducing structural and strength adaptations in the contralateral untrained limb. However, most of these adaptations were lost after 6 weeks of detraining. CLINICAL RELEVANCE: Our study suggests that cross-education training can be of great importance for clinical application and musculoskeletal and neuromuscular rehabilitative processes after unilateral injury.',\n",
       " 'sentiment': 'Yes'}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "results_abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "180\n19\n"
     ]
    }
   ],
   "source": [
    "train_rows = int(np.floor(len(results_abstracts)*0.9))\n",
    "train_data = results_abstracts[:train_rows]\n",
    "test_data = results_abstracts[(train_rows+1):]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pubmed Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping sentences with their Labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(180, 180, 19, 19)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
    "\n",
    "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualizing one of the sentences from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' BACKGROUND: Cross-education of strength refers to the strength gain that is transferred to the contralateral limb after a unilateral training program. HYPOTHESIS: Unilateral eccentric training using different muscle contraction times would improve the structural and functional properties of the untrained contralateral limb. STUDY DESIGN: Randomized controlled trial. LEVEL OF EVIDENCE: Level 2. METHODS: Thirty-six participants were randomized into a control group, experimental group 1 (EG6s; eccentric contraction runtime = 6 seconds) and experimental group 2 (EG3s; eccentric contraction runtime = 3 seconds). The thickness and elastographic index of the patellar tendon (PT), lean mass and fat percentage of the thigh, contractile properties of the vastus lateralis (VL), as well as isometric, concentric, and eccentric knee extensor peak torques, and eccentric single-leg decline squat (SLDSe) 1 repetition maximum (1-RM) were measured after 6 weeks of SLDSe training (3 times per week, 80% of 1-RM) and after 6 weeks of detraining in the untrained contralateral limb. RESULTS: After training, there was an increase in lean thigh mass of the untrained limb in both groups: EG6s (0.17 ± 0.29 kg;P = 0.03; effect size [ES] = 0.15) and EG3s (0.15 ± 0.23 kg; P = 0.04; ES = 0.19). Likewise, both EG6s (62.30 ± 19.09 kg; P < 0.001; ES = 4.23) and EG3s (68.09 ± 27.49 kg; P < 0.001; ES = 3.40) increased their 1-RM, isometric (EG6s: 48.64 ± 44.82 N·m, P < 0.001, ES = 0.63; EG3s: 34.81 ± 47.30 N·m, P = 0.004, ES = 0.38), concentric at 60 deg/s and 180 deg/s and eccentric at 60 deg/s and 180 deg/s knee extensor peak torques (P < 0.05) in the untrained limb. However, no differences were found in the contractile properties of the VL or in the thickness of the PT after eccentric training in either of the 2 experimental groups. CONCLUSION: Regardless of the runtime of the contraction, 6 weeks of SLSDe was effective for inducing structural and strength adaptations in the contralateral untrained limb. However, most of these adaptations were lost after 6 weeks of detraining. CLINICAL RELEVANCE: Our study suggests that cross-education training can be of great importance for clinical application and musculoskeletal and neuromuscular rehabilitative processes after unilateral injury.'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Token embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(180, 19)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
    "\n",
    "len(train_tokens), len(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((180, 512), (19, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_tokens_ids.shape, test_tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((180,), (19,), 0.7611111111111111, 0.6842105263157895)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_y = np.array(train_labels) == 'Yes'\n",
    "test_y = np.array(test_labels) == 'Yes'\n",
    "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking few random IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# ensuring that the model runs on GPU, not on CPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'0.0M'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'439.065088M'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
    "y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n",
    "x.shape, y.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.3978583 ],\n",
       "       [0.3872686 ],\n",
       "       [0.41915417]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "y = bert_clf(x)\n",
    "y.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'6697.349632M'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'439.065088M'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "y, x, pooled = None, None, None\n",
    "torch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyper-parameters\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'439.065088M'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [03:53<00:00, 23.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in tqdm( range(EPOCHS) ):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        \n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\n",
    "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in tqdm( enumerate(test_dataloader) ):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n"
   ]
  },
  {
   "source": [
    "# Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "bert_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9385141"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "all_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'Free radicals, or reactive oxygen species, have been implicated as one of the primary causes of myocardial pathologies elicited by chronic diseases and age. The imbalance between pro-oxidants and antioxidants, termed \"oxidative stress\", involves several pathological changes in mouse hearts, including hypertrophy and cardiac dysfunction. However, the molecular mechanisms and adaptations of the hearts in mice lacking cytoplasmic superoxide dismutase (Sod1KO) have not been investigated. We used echocardiography to characterize cardiac function and morphology in vivo. Protein expression and enzyme activity of Sod1KO were confirmed by targeted mass spectrometry and activity gel. The heart weights of the Sod1KO mice were significantly increased compared with their wildtype peers. The increase in heart weights was accompanied by concentric hypertrophy, posterior wall thickness of the left ventricles (LV), and reduced LV volume. Activated downstream pathways in Sod1KO hearts included serine-threonine kinase and ribosomal protein synthesis. Notably, the reduction in LV volume was compensated by enhanced systolic function, measured by increased ejection fraction and fractional shortening. A regulatory sarcomeric protein, troponin I, was hyper-phosphorylated in Sod1KO, while the vinculin protein was upregulated. In summary, mice lacking cytoplasmic superoxide dismutase were associated with an increase in heart weights and concentric hypertrophy, exhibiting a pathological adaptation of the hearts to oxidative stress.',\n",
       " 'sentiment': 'Yes'}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "test_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}